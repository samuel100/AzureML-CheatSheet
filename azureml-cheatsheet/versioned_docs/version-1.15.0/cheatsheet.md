---
title: Cheat Sheet
---

### Basic setup

**Connect to workspace.**

```python
from azureml.core import Workspace
ws = Workspace.from_config()
```

For more details: [Workspaces](workspace)

**Connect to compute target.**

```python
from azureml.core import ComputeTarget
target = ComputeTarget(ws, '<compute-target-name>')
```

For more details: [Compute Target](compute-targets)

**Prepare Python environment.**  
You can use a `requirements.txt` file to define a Python environment on your compute.

```python
from azureml.core import Environment
env = Environment.from_pip_requirements('<environment-name>', '<path/to/requirements.txt>')
```

You can also use conda environments and docker images to prepare your environments.  

For more details: [Environment](environment)


### Submit code

To run code in AML you need to:

1. **Configure**: Configuration includes specifying the code to run, the compute
target to run on and the Python environment to run in.
2. **Submit**: Create or reuse an AML Experiment and submit the run.

#### ScriptRunConfig (Configure)

A typical directory may have the following structure:

```bash
source_directory/
    script.py    # entry point to your code
    module1.py   # modules called by script.py     
    ...
```

To run `$ python script.py` on a remote compute cluster `target: ComputeTarget` with an
environment `env: Environment` we use the `ScriptRunConfig` class.

```python
from azureml.core import ScriptRunConfig

config = ScriptRunConfig(
    source_directory='<path/to/source_directory>',  # relative paths okay
    script='script.py',
    compute_target=target,
    environment=env,
)
```

:::info
- `compute_target`: If not provided the script will run on your local machine **TODO: does this require docker?**
- `environment`: If not provided, uses a default Python environment managed by Azure ML (azureml.core.runconfig.DEFAULT_CPU_IMAGE) **TODO: provide details on this image**.
:::

#### Experiment (Submit)

To submit this code we create an `Experiment`: a light-weight container that helps to
oraganize our submissions and keep track of code (See [Run History](run-history)).

```python
exp = Experiment(ws, '<experiment-name>')
run = exp.submit(config)
print(run.get_portal_url())
```

This link will take you to the Azure ML Studio where you can monitor your run.

For more details: [ScriptRunConfig](script-run-config)





## AML Assets

#### [Workspace](workspace)

Instantiate `Workspace` object used to connect to your AML assets.

```python
from azureml.core import Workspace
ws = Workspace(
    subscription_id="<subscription_id>",
    resource_group="<resource_group>",
    workspace_name="<workspace_name>",
)
```

For convenience store your credentials in a `config.json`:

```python
from azureml.core import Workspace
ws.write_config()
# write config.json file with your AML credentials

ws = Workspace.from_config()
# read your aml credentials from config.json and instantiate
# Workspace object
```

_[VS Code snippet](vs-code-snippets/snippets#create-aml-workspace-from-config)_

Workspaces are a foundational object used throughout AML and are used in the
constructors of many other classes. In the following examples we frequently
omit the workspace object instantiation and simply refer to `ws`.

See the [workspaces](workspace) page for more ways to instantiate a
workspace.

#### Experiment

An experiment is used as an organizational principle, storing run history and
tracking metrics.

Get or create an experiment:

```python
from azureml.core import Experiment
exp = Experiment(ws, "<experiment-name>")
```

#### Run

A run represents a single trial of an experiment.

Runs are used to monitor the asynchronous execution of a trial, log metrics and
store output of the trial, and to analyze results and access artifacts
generated by the trial.

To get all the runs associated with a given experiment.

```python
from azureml.core import Experiment, Run
exp = Experiment(ws, "<experiment-name>")
runs = exp.get_runs()
# generator of the runs in reverse chronological order
```

:::tip "Run IDs"

    A unique `run_id` is automatically generated for each run.
:::

There are multiple ways to create a run. For example:

##### From configuration

To submit a run - either to local compute or to cloud
compute - define a configuration. See
"[Running Scripts in AML](#running-scripts-in-aml)" for details.

```python
from azureml.core import Experiment
from azureml.core import ScriptRunConfig

exp = Experiment(ws, "<experiment-name>")
config = ScriptRunConfig(...)  # provide code, compute, environments, etc.
run = exp.submit(config)    
```

##### Interactive run

Interactive runs are typically used from a Jupyter notebook.

```python
from azureml.core import Experiment
exp = Experiment(ws, "<experiment-name>")  # get or create experiment

run = exp.start_logging()                  # start interactive run
result = my_function()                     # make some calculation
run.log('result', result)                  # log the results to the run
run.complete()                             # stop the interactive run
```

#### [Compute Target](compute-targets)

Compute targets are an AML abstraction around the concept of a compute resource.
This can range from your local machine to a cluster of Azure VMs.

To use an existing compute target:

```python
from azureml.core import ComputeTarget
compute_target = ComputeTarget(workspace=ws, name="<compute-name>")
```

For example, to create a new cluster of between 0 and 4 "Standard_NC24rs_v3"
VMs,

```python
from azureml.core import ComputeTarget
from azureml.core.compute import AmlCompute
compute_config = AmlCompute.provisioning_configuration(
    vm_size="Standard_NC24rs_v3",
    min_nodes=0,
    max_nodes=4,
)
compute_target = ComputeTarget.create(ws, "<compute-name>", compute_config)
compute_target.wait_for_completion(show_output=True)
```

See the [Compute Targets](compute-targets) page for more examples.

#### [Environment](environment)

The `Environment` class is used to configure reproducable Python environments
for use throughout AML. These environments are versioned and sharable.

**Creating AML Environments.**

##### From existing Conda environment

Create an AML Environment from an existing local conda environment:

```python
from azureml.core import Environment
env = Environment.from_existing_conda_environment(
    name="<aml-environment-name>",
    conda_environment_name="<conda_environment_name>",
)
```

:::info "Conda local environments"
To see your local Conda environments run

```bash
conda env list
```

For more information, see
[Managing environments](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html)
in the conda user guide.
:::

##### From Conda `env.yml` file

Create an AML Environment from a conda `env.yml` file:

```python
from azureml.core import Environment
my_env = Environment.from_conda_specification(
    name='<environment-name>',
    file_path='<path-to-env.yml>',   # relative paths are okay
)
```

:::info "Configure conda environments with `env.yml` files"
Conda environments can be specified by way of an `env.yml` file, e.g.

```yml
name: example
dependencies:
    - numpy
    - pandas
```

For more information, see
[Managing environments](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html)
in the conda user guide.
:::

##### From pip `requirements.txt` file

To create an AML environment from a pip `requirement.txt` file:

```python
from azureml.core import Environment
env = Environment.from_pip_specification(
    name='<environment-name>',
    file_path='<path-to-requirements.txt>',
)
```

:::info "Configure pip environments with `requirements.txt` files"
Requirements files are pip's way configuring Python environments. For more
information, see
[Requirements file](https://pip.pypa.io/en/latest/user_guide/#requirements-files)
in the pip user guide.
:::

**Registering AML Environments.**

To register this environment with the workspace

```python
env.register(ws) # can be accessed by any user of the workspace
```

To view all environments registered to a workspace

```python
from azureml.core import Environment
registered_environments = Environment.list(ws)
```

#### [Datastore](datastore)

Each workspace comes with a default datastore.

```python
ds = ws.get_default_datastore()
```

Any datastore that is registered to workspace can be accessed by name.

```python
from azureml.core import Datastore
ds = Datastore.get(ws, "<name-of-registered-datastore>")
```

To register a store via a SAS token:

```python
ds = Datastore.register_azure_blob_container(
    workspace=ws,
    datastore_name="<datastore-name>",
    container_name="<container-name>",
    account_name="<account-name>",
    sas_token="<sas-token>",
)
```

_[VS Code snippet](vs-code-snippets/snippets#register-azure-blob-container-from-sas)_

For more ways authentication options and for different underlying storage see
the AML documentation on
[Datastores](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore(class)?view=azure-ml-py).


## Running Scripts in AML

To run code in AML you need to:

1. **Configure**: Configuration includes specifying the code to run, the compute
target to run on and the Python environment to run in.
2. **Submit**: Create or reuse an AML Experiment and submit the run.

#### ScriptRunConfig

A typical directory may have the following structure:

```bash
source_directory/
    script.py    # entry point to your code
    module1.py   # modules called by script.py     
    ...
```

**ScriptRunConfig: Basic**

```python
from azureml.core import ScriptRunConfig

config = ScriptRunConfig(
    source_directory='<path/to/source_directory>',  # relative paths okay
    script='script.py',
)
```

**ScriptRunConfig:Command line arguments**

`script.py` may be set up to accept command line arguments, e.g.,

```bash
python script.py --argument 42 --another_argument 73
```

In that case we pass the arguments list to `config`:

```python
from azureml.core import ScriptRunConfig

config = ScriptRunConfig(
    source_directory='<path/to/source_directory>',  # relative paths okay
    script='script.py',
    arguments=[
        'argument', 42,
        'another_argument', 73,
    ],
)
```

**ScriptRunConfig: ComputeTarget**

By default this configuration will run on local compute.

To use AML compute target specify in the `run_config` attribute:

```python
from azureml.core import ComputeTarget
compute_target = ComputeTarget(workspace=ws, name="<compute-name>")
config.run_config.target = target # of type azureml.core.ComputeTarget
```

See [Compute Targets](compute-targets) for creating and accessing
AML ComputeTargets.

**ScriptRunConfig: Environment**

If your script requires a specific Python environment to run: specify in
the `run_config` attribute:

```python
from azureml.core import Environment
env = Environment(ws, '<environment-name>')
config.run_config.environment = env # of type azureml.core.Environment
```

See [Environment](environment) for managing AML Environments.

**ScriptRunConfig: Submit**

Submit this configuration to run as part of an Experiment.

```python
from azureml.core import Experiment
exp = Experiment(ws, "<experiment-name>")   # get or create Experiment
run = exp.submit(config)                    # submit config to run in AML
# returns Run object
```

## Logging

#### Logging metrics

To log metrics in your running script add the following:

```python
from azureml.core import Run
run = Run.get_context()
run.log("metric-name", metric_value)
```

#### Viewing metrics with the Python SDK

Viewing metrics in a run

```python
metrics = run.get_metrics()
# metrics is of type Dict[str, List[float]] mapping mertic names
# to a list of the values for that metric in the given run.

metrics.get("metric-name")
# list of metrics in the order they were recorded
```

To view all recorded values for a given metric `my-metric` in a
given experiment `my-experiment`:

```python
experiments = ws.experiments
# of type Dict[str, Experiment] mapping experiment names the
# corresponding Experiment

exp = experiments['my-experiment']
for run in exp.get_runs():
    metrics = run.get_metrics()
    
    my_metric = metrics.get('my-metric')
    if my_metric:
        print(my_metric)
```