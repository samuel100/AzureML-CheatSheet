# Azure ML Cheat Sheet

## AML Assets

#### [Workspace](more/workspace.md)

Instantiate `Workspace` object used to connect to your AML assets.

```python
from azureml.core import Workspace
ws = Workspace(
    subscription_id="<subscription_id>",
    resource_group="<resource_group>",
    workspace_name="<workspace_name>",
)
```

For convenience store your credentials in a `config.json`:

```python
from azureml.core import Workspace
ws.write_config()
# write config.json file with your AML credentials

ws = Workspace.from_config()
# read your aml credentials from config.json and instantiate
# Workspace object
```

_[VS Code snippet](vs-code-snippets/snippets#create-aml-workspace-from-config)_

Workspaces are a foundational object used throughout AML and are used in the
constructors of many other classes. In the following examples we frequently
omit the workspace object instantiation and simply refer to `ws`.

See the [workspaces](more/workspace.md) page for more ways to instantiate a
workspace.

#### Experiment

An experiment is used as an organizational principle, storing run history and
tracking metrics.

Get or create an experiment:

```python
from azureml.core import Experiment
exp = Experiment(ws, "<experiment-name>")
```

#### Run

A run represents a single trial of an experiment.

Runs are used to monitor the asynchronous execution of a trial, log metrics and
store output of the trial, and to analyze results and access artifacts
generated by the trial.

To get all the runs associated with a given experiment.

```python
from azureml.core import Experiment, Run
exp = Experiment(ws, "<experiment-name>")
runs = exp.get_runs()
# generator of the runs in reverse chronological order
```

??? note "Run IDs"

    A unique `run_id` is automatically generated for each run.

There are multiple ways to create a run. For example:

=== "Interactive run"

    Create interactive runs in an interactive scenario e.g. Jupyter notebooks.

    ```python
    from azureml.core import Experiment
    exp = Experiment(ws, "<experiment-name>")  # get or create experiment
    
    run = exp.start_logging()                  # start interactive run
    result = my_function()                     # make some calculation
    run.log('result', result)                  # log the results to the run
    run.complete()                             # stop the interactive run
    ```

=== "From configuration"

    To submit a run asynchronously - either to local compute or to cloud
    compute - define a configuration. See
    "[Running Scripts in AML](#running-scripts-in-aml)" for details.

    ```python
    from azureml.core import Experiment
    from azureml.core import ScriptRunConfig
    
    exp = Experiment(ws, "<experiment-name>")
    config = ScriptRunConfig(...)  # provide code, compute, environments, etc.
    run = exp.submit(config)    
    ```

#### [Compute Target](more/compute-targets.md)

Compute targets are an AML abstraction around the concept of a compute resource.
This can range from your local machine to a cluster of Azure VMs.

To use an existing compute target:

```python
from azureml.core import ComputeTarget
compute_target = ComputeTarget(workspace=ws, name="<compute-name>")
```

For example, to create a new cluster of between 0 and 4 "Standard_NC24rs_v3"
VMs,

```python
from azureml.core import ComputeTarget
from azureml.core.compute import AmlCompute
compute_config = AmlCompute.provisioning_configuration(
    vm_size="Standard_NC24rs_v3",
    min_nodes=0,
    max_nodes=4,
)
compute_target = ComputeTarget.create(ws, "<compute-name>", compute_config)
compute_target.wait_for_completion(show_output=True)
```

See the [Compute Targets](more/compute-targets.md) page for more examples.

#### [Environment](more/environment.md)

The `Environment` class is used to configure reproducable Python environments
for use throughout AML. These environments are versioned and sharable.

**Creating AML Environments.**

=== "From existing Conda environment"

    Create an AML Environment from an existing local conda environment:

    ```python
    from azureml.core import Environment
    env = Environment.from_existing_conda_environment(
        name="<aml-environment-name>",
        conda_environment_name="<conda_environment_name>",
    )
    ```

    ??? "Conda local environments"
        To see your local Conda environments run
    
        ```bash
        conda env list
        ```
    
        For more information, see
        [Managing environments](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html)
        in the conda user guide.

=== "From Conda `env.yml` file"

    Create an AML Environment from a conda `env.yml` file:

    ```python
    from azureml.core import Environment
    my_env = Environment.from_conda_specification(
        name='<environment-name>',
        file_path='<path-to-env.yml>',   # relative paths are okay
    )
    ```

    ??? "Configure conda environments with `env.yml` files"
        Conda environments can be specified by way of an `env.yml` file, e.g.
    
        ```yml
        name: example
        dependencies:
          - numpy
          - pandas
        ```

        For more information, see
        [Managing environments](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html)
        in the conda user guide.

=== "From pip `requirements.txt` file"

    To create an AML environment from a pip `requirement.txt` file:
    
    ```python
    from azureml.core import Environment
    env = Environment.from_pip_specification(
        name='<environment-name>',
        file_path='<path-to-requirements.txt>',
    )
    ```

    ??? "Configure pip environments with `requirements.txt` files"
        Requirements files are pip's way configuring Python environments. For more
        information, see
        [Requirements file](https://pip.pypa.io/en/latest/user_guide/#requirements-files)
        in the pip user guide.


**Registering AML Environments.**

To register this environment with the workspace

```python
env.register(ws) # can be accessed by any user of the workspace
```

To view all environments registered to a workspace

```python
from azureml.core import Environment
registered_environments = Environment.list(ws)
```

#### [Datastore](more/datastore.md)

Each workspace comes with a default datastore.

```python
ds = ws.get_default_datastore()
```

Any datastore that is registered to workspace can be accessed by name.

```python
from azureml.core import Datastore
ds = Datastore.get(ws, "<name-of-registered-datastore>")
```

To register a store via a SAS token:

```python
ds = Datastore.register_azure_blob_container(
    workspace=ws,
    datastore_name="<datastore-name>",
    container_name="<container-name>",
    account_name="<account-name>",
    sas_token="<sas-token>",
)
```

_[VS Code snippet](vs-code-snippets/snippets#register-azure-blob-container-from-sas)_

For more ways authentication options and for different underlying storage see
the AML documentation on
[Datastores](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore(class)?view=azure-ml-py).


## Running Scripts in AML

To run code in AML you need to:

1. **Configure**: Configuration includes specifying the code to run, the compute
target to run on and the Python environment to run in.
2. **Submit**: Create or reuse an AML Experiment and submit the run.

#### ScriptRunConfig

A typical directory may have the following structure:

```bash
source_directory/
    script.py    # entry point to your code
    module1.py   # modules called by script.py     
    ...
```

**ScriptRunConfig: Basic**

```python
from azureml.core import ScriptRunConfig

config = ScriptRunConfig(
    source_directory='<path/to/source_directory>',  # relative paths okay
    script='script.py',
)
```

**ScriptRunConfig:Command line arguments**

`script.py` may be set up to accept command line arguments, e.g.,

```bash
python script.py --argument 42 --another_argument 73
```

In that case we pass the arguments list to `config`:

```python
from azureml.core import ScriptRunConfig

config = ScriptRunConfig(
    source_directory='<path/to/source_directory>',
    script='script.py',
    arguments=[
        'argument', 42,
        'another_argument', 73,
    ],
)
```

**ScriptRunConfig: ComputeTarget**

By default this configuration will run on local compute.

To use AML compute target specify in the `run_config` attribute:

```python
from azureml.core import ComputeTarget
compute_target = ComputeTarget(workspace=ws, name="<compute-name>")
config.run_config.target = target # of type azureml.core.ComputeTarget
```

See [Compute Targets](more/compute-targets.md) for creating and accessing
AML ComputeTargets.

**ScriptRunConfig: Environment**

If your script requires a specific Python environment to run: specify in
the `run_config` attribute:

```python
from azureml.core import Environment
env = Environment(ws, '<environment-name>')
config.run_config.environment = env # of type azureml.core.Environment
```

See [Environment](more/environment.md) for managing AML Environments.

**ScriptRunConfig: Submit**

Submit this configuration to run as part of an Experiment.

```python
from azureml.core import Experiment
exp = Experiment(ws, "<experiment-name>")   # get or create Experiment
run = exp.submit(config)                    # submit config to run in AML
# returns Run object
```

## Logging

#### Logging metrics

To log metrics in your running script add the following:

```python
from azureml.core import Run
run = Run.get_context()
run.log("metric-name", metric_value)
```

#### Viewing metrics with the Python SDK

Viewing metrics in a run

```python
metrics = run.get_metrics()
# metrics is of type Dict[str, List[float]] mapping mertic names
# to a list of the values for that metric in the given run.

metrics.get("metric-name")
# list of metrics in the order they were recorded
```

To view all recorded values for a given metric `my-metric` in a
given experiment `my-experiment`:

```python
experiments = ws.experiments
# of type Dict[str, Experiment] mapping experiment names the
# corresponding Experiment

exp = experiments['my-experiment']
for run in exp.get_runs():
    metrics = run.get_metrics()
    
    my_metric = metrics.get('my-metric')
    if my_metric:
        print(my_metric)
```